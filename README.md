William Stetar

*Building tools and frameworks to understand how language models fail‚Äîand why it matters for alignment.*

**AI Safety Researcher | Computational Linguist | Systems Developer**

I analyze language model behavior through computational linguistics, focusing on how discourse patterns shape alignment failures. I also build tools for LLM integration, git automation, and symbolic reasoning systems.
Current Research: Investigating epistemic failure modes in LLMs using Systemic Functional Linguistics‚Äîspecifically how models prioritize interpersonal coherence over factual accuracy under pressure.
Technical Skills: Go, Python, Bash, C#, SQL | LLM integration | CLI tooling | Git automation

<details>
<summary>Projects</summary>

- [PRbuddy](github.com/soyuz43/PRbuddy) - Auto-generates pull request drafts using Git hooks and LLM infrastructure

- [Symbolic Grammar Interpreter](github.com/soyuz43/Symbolic-Grammar-Interpreter) - Recursive system for analyzing contradiction and structural drift in text

- [Holoplan CLI](github.com/soyuz43/holoplan-cli) - Transforms user stories into Draw.io wireframes via multi-agent LLM reasoning


</details>

<details>
<summary>Writing & Research</summary>

üìù [Hashnode Blog](https://copin43.hashnode.dev/) - Essays on AI safety, linguistic analysis, and ethical implications of LLMs

Notable pieces:

- "The Nuremberg Defense of AI" - On accountability in ML development

- "Epistemic Autoimmunity" - Framework for analyzing alignment failures through discourse patterns

</details>

<details>
<summary>Currently</summary>

- Conducting comparative analysis of Western vs. Eastern LLM failure modes
- Developing empirical framework for measuring epistemic autoimmunity in model outputs
- Open to collaboration and full-time research positions in AI safety

</details>
<details>
<summary>Contact</summary>

üìß kebekad673@proton.me
üîó [LinkedIn](https://www.linkedin.com/in/copin43)

</details>
